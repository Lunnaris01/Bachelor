{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juull100/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/juull100/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from AdversialAttackAtariEnvironment import *\n",
    "from AtariEnvironment import *\n",
    "from ACERNetwork import * \n",
    "from ReplayBuffer import *\n",
    "from Worker import * \n",
    "from Trainer import train,train_acer, test\n",
    "import gym as gym\n",
    "from time import sleep\n",
    "from time import time\n",
    "from utils import preprocess\n",
    "\n",
    "network_t = \"cnn\"\n",
    "GAME_NAME=\"Seaquest-v0\"\n",
    "#GAME_NAME = \"LunarLander-v2\"\n",
    "TB_DIR = \"saves/seaquest/S1/\"\n",
    "env=Atari_Environment(gym.make(GAME_NAME))\n",
    "#env = gym.make(GAME_NAME)\n",
    "state= env.reset()\n",
    "training_finished = False\n",
    "LEARNING_RATE = 8e-5\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_method(game_name=GAME_NAME, num_agents = 1, restore = True, save_path=TB_DIR):\n",
    "        \n",
    "    envs=[]\n",
    "    agents=[]\n",
    "    processes=[]\n",
    "    \n",
    "    for _ in range(num_agents):\n",
    "        #a_env = gym.make(game_name)\n",
    "        #a_env = Custom_Environment(gym.make(game_name))\n",
    "        a_env = Atari_Environment(gym.make(game_name), clipreward=False)\n",
    "        envs.append(a_env)\n",
    "       \n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        with tf.device('/cpu:0'):\n",
    "            coord = tf.train.Coordinator()\n",
    "            G_Agent=ACERNetwork(sess,envs[0],\"global\",network_type=network_t,lr=LEARNING_RATE,decay=0.99)\n",
    "            for agent_num in range(num_agents):\n",
    "                agents.append(ACERNetwork(sess,envs[0],str(agent_num),G_Agent,network_type=network_t))\n",
    "            saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "            if restore:\n",
    "                ckpt = tf.train.get_checkpoint_state(save_path+\"checkpoints\")\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "            for thread_id in range(num_agents):\n",
    "                processes.append(threading.Thread(target=test, args=(agents[thread_id],\n",
    "                envs[thread_id], 200, False)))\n",
    "\n",
    "            for p in processes:\n",
    "                p.daemon = True\n",
    "                p.start()\n",
    "\n",
    "                \n",
    "            for p in processes:\n",
    "                p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saves/seaquest/S1/checkpoints/model825673.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juull100/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/juull100/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run  0 Reward:  1740.0\n",
      "Run  1 Reward:  1760.0\n",
      "Run  2 Reward:  1740.0\n",
      "Run  3 Reward:  1760.0\n",
      "Run  4 Reward:  1760.0\n",
      "Run  5 Reward:  1800.0\n",
      "Run  6 Reward:  1720.0\n",
      "Run  7 Reward:  1740.0\n",
      "Run  8 Reward:  1740.0\n",
      "Run  9 Reward:  1740.0\n",
      "Run  10 Reward:  1760.0\n",
      "Run  11 Reward:  1740.0\n",
      "Run  12 Reward:  1340.0\n",
      "Run  13 Reward:  1680.0\n",
      "Run  14 Reward:  1740.0\n",
      "Run  15 Reward:  1740.0\n",
      "Run  16 Reward:  1760.0\n",
      "Run  17 Reward:  1760.0\n",
      "Run  18 Reward:  1740.0\n",
      "Run  19 Reward:  1760.0\n",
      "Run  20 Reward:  1760.0\n",
      "Run  21 Reward:  1720.0\n",
      "Run  22 Reward:  1720.0\n",
      "Run  23 Reward:  1360.0\n",
      "Run  24 Reward:  1740.0\n",
      "Run  25 Reward:  1760.0\n",
      "Run  26 Reward:  1740.0\n",
      "Run  27 Reward:  1720.0\n",
      "Run  28 Reward:  1800.0\n",
      "Run  29 Reward:  1740.0\n",
      "Run  30 Reward:  1760.0\n",
      "Run  31 Reward:  1720.0\n",
      "Run  32 Reward:  1700.0\n",
      "Run  33 Reward:  1740.0\n",
      "Run  34 Reward:  1760.0\n",
      "Run  35 Reward:  1740.0\n",
      "Run  36 Reward:  1740.0\n",
      "Run  37 Reward:  1720.0\n",
      "Run  38 Reward:  1760.0\n",
      "Run  39 Reward:  1740.0\n",
      "Run  40 Reward:  1760.0\n",
      "Run  41 Reward:  1760.0\n",
      "Run  42 Reward:  1760.0\n",
      "Run  43 Reward:  1780.0\n",
      "Run  44 Reward:  1740.0\n",
      "Run  45 Reward:  1760.0\n",
      "Run  46 Reward:  1720.0\n",
      "Run  47 Reward:  1700.0\n",
      "Run  48 Reward:  1720.0\n",
      "Run  49 Reward:  1740.0\n",
      "Run  50 Reward:  1780.0\n",
      "Run  51 Reward:  1720.0\n",
      "Run  52 Reward:  1740.0\n",
      "Run  53 Reward:  1780.0\n",
      "Run  54 Reward:  1740.0\n",
      "Run  55 Reward:  1780.0\n",
      "Run  56 Reward:  1740.0\n",
      "Run  57 Reward:  1780.0\n",
      "Run  58 Reward:  1740.0\n",
      "Run  59 Reward:  1720.0\n",
      "Run  60 Reward:  1700.0\n",
      "Run  61 Reward:  1760.0\n",
      "Run  62 Reward:  1780.0\n",
      "Run  63 Reward:  1760.0\n",
      "Run  64 Reward:  1760.0\n",
      "Run  65 Reward:  1780.0\n",
      "Run  66 Reward:  1780.0\n",
      "Run  67 Reward:  1760.0\n",
      "Run  68 Reward:  1780.0\n",
      "Run  69 Reward:  1720.0\n",
      "Run  70 Reward:  1700.0\n",
      "Run  71 Reward:  1740.0\n",
      "Run  72 Reward:  1760.0\n",
      "Run  73 Reward:  1780.0\n",
      "Run  74 Reward:  1760.0\n",
      "Run  75 Reward:  1720.0\n",
      "Run  76 Reward:  1760.0\n",
      "Run  77 Reward:  1760.0\n",
      "Run  78 Reward:  1760.0\n",
      "Run  79 Reward:  1760.0\n",
      "Run  80 Reward:  1740.0\n",
      "Run  81 Reward:  1780.0\n",
      "Run  82 Reward:  1780.0\n",
      "Run  83 Reward:  1760.0\n",
      "Run  84 Reward:  1780.0\n",
      "Run  85 Reward:  1720.0\n",
      "Run  86 Reward:  1720.0\n",
      "Run  87 Reward:  1740.0\n",
      "Run  88 Reward:  1720.0\n",
      "Run  89 Reward:  1760.0\n",
      "Run  90 Reward:  1780.0\n",
      "Run  91 Reward:  1740.0\n",
      "Run  92 Reward:  1760.0\n",
      "Run  93 Reward:  1760.0\n",
      "Run  94 Reward:  1760.0\n",
      "Run  95 Reward:  1580.0\n",
      "Run  96 Reward:  1720.0\n",
      "Run  97 Reward:  1720.0\n",
      "Run  98 Reward:  1740.0\n",
      "Run  99 Reward:  1760.0\n",
      "Run  100 Reward:  1760.0\n",
      "Run  101 Reward:  1740.0\n",
      "Run  102 Reward:  1740.0\n",
      "Run  103 Reward:  1780.0\n",
      "Run  104 Reward:  1720.0\n",
      "Run  105 Reward:  1780.0\n",
      "Run  106 Reward:  1760.0\n",
      "Run  107 Reward:  1740.0\n",
      "Run  108 Reward:  1740.0\n",
      "Run  109 Reward:  1780.0\n",
      "Run  110 Reward:  1740.0\n",
      "Run  111 Reward:  1800.0\n",
      "Run  112 Reward:  1740.0\n",
      "Run  113 Reward:  1780.0\n",
      "Run  114 Reward:  1760.0\n",
      "Run  115 Reward:  1780.0\n",
      "Run  116 Reward:  1700.0\n",
      "Run  117 Reward:  1700.0\n",
      "Run  118 Reward:  1720.0\n",
      "Run  119 Reward:  1700.0\n",
      "Run  120 Reward:  1780.0\n",
      "Run  121 Reward:  1740.0\n",
      "Run  122 Reward:  1700.0\n",
      "Run  123 Reward:  1720.0\n",
      "Run  124 Reward:  1720.0\n",
      "Run  125 Reward:  1780.0\n",
      "Run  126 Reward:  1680.0\n",
      "Run  127 Reward:  1720.0\n",
      "Run  128 Reward:  1740.0\n",
      "Run  129 Reward:  1760.0\n",
      "Run  130 Reward:  1780.0\n",
      "Run  131 Reward:  1760.0\n",
      "Run  132 Reward:  1780.0\n",
      "Run  133 Reward:  1720.0\n",
      "Run  134 Reward:  1740.0\n",
      "Run  135 Reward:  1760.0\n",
      "Run  136 Reward:  1800.0\n",
      "Run  137 Reward:  1740.0\n",
      "Run  138 Reward:  1740.0\n",
      "Run  139 Reward:  1760.0\n",
      "Run  140 Reward:  1740.0\n",
      "Run  141 Reward:  1760.0\n",
      "Run  142 Reward:  1760.0\n",
      "Run  143 Reward:  1740.0\n",
      "Run  144 Reward:  1740.0\n",
      "Run  145 Reward:  1700.0\n",
      "Run  146 Reward:  1740.0\n",
      "Run  147 Reward:  1720.0\n",
      "Run  148 Reward:  1360.0\n",
      "Run  149 Reward:  1740.0\n",
      "Run  150 Reward:  1780.0\n",
      "Run  151 Reward:  1720.0\n",
      "Run  152 Reward:  1740.0\n",
      "Run  153 Reward:  1760.0\n",
      "Run  154 Reward:  1740.0\n",
      "Run  155 Reward:  1740.0\n",
      "Run  156 Reward:  1800.0\n",
      "Run  157 Reward:  1760.0\n",
      "Run  158 Reward:  1740.0\n",
      "Run  159 Reward:  1740.0\n",
      "Run  160 Reward:  1780.0\n",
      "Run  161 Reward:  1760.0\n",
      "Run  162 Reward:  1740.0\n",
      "Run  163 Reward:  1740.0\n",
      "Run  164 Reward:  1740.0\n",
      "Run  165 Reward:  1700.0\n",
      "Run  166 Reward:  1740.0\n",
      "Run  167 Reward:  1700.0\n",
      "Run  168 Reward:  1740.0\n",
      "Run  169 Reward:  1740.0\n",
      "Run  170 Reward:  1760.0\n",
      "Run  171 Reward:  1780.0\n",
      "Run  172 Reward:  1780.0\n",
      "Run  173 Reward:  1720.0\n",
      "Run  174 Reward:  1760.0\n",
      "Run  175 Reward:  1780.0\n",
      "Run  176 Reward:  1760.0\n",
      "Run  177 Reward:  1760.0\n",
      "Run  178 Reward:  1780.0\n",
      "Run  179 Reward:  1800.0\n",
      "Run  180 Reward:  1740.0\n",
      "Run  181 Reward:  1760.0\n",
      "Run  182 Reward:  1740.0\n",
      "Run  183 Reward:  1780.0\n",
      "Run  184 Reward:  1740.0\n",
      "Run  185 Reward:  1780.0\n",
      "Run  186 Reward:  1760.0\n",
      "Run  187 Reward:  1740.0\n",
      "Run  188 Reward:  1740.0\n",
      "Run  189 Reward:  1740.0\n",
      "Run  190 Reward:  1760.0\n",
      "Run  191 Reward:  1740.0\n",
      "Run  192 Reward:  1740.0\n",
      "Run  193 Reward:  1780.0\n",
      "Run  194 Reward:  1800.0\n",
      "Run  195 Reward:  1760.0\n",
      "Run  196 Reward:  1800.0\n",
      "Run  197 Reward:  1760.0\n",
      "Run  198 Reward:  1740.0\n",
      "Run  199 Reward:  1760.0\n",
      "avg of 200  runs:  1742.2\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "test_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
