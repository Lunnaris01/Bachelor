\section{Introduction}\raggedbottom 
\citet{Sut98} describe the reinforcement learning  task as "learning what to do".
Acting optimal within an unknown environment can be very difficult.
The field within machine learning addressing this problem is called reinforcement learning.

The reinforcement problem consists of an \textit{agent} taking \textit{actions} within some sort of \textit{environment}.
By interacting with the \textit{environment} the \textit{agent} tries to find the \textit{actions} which will yield the most \textit{reward} in the future.

The goal of reinforcement learning is to create fast and reliable learning algorithms for the \textit{agent} to take the optimal \textit{actions} within the \textit{environment}.
This means, we want to achieve the maximum possible \textit{reward} within an episode or over a period of time if an environment is continuous.

Environments pose a lot of different tasks of varying difficulty.
Easy environments, like the 'Cartpole'-environment, require the agent to simply balance a pole based on 4 input values with only 2 possible actions, whereas more complex and demanding tasks might have high dimensional images as states, with many possible actions to choose from.

This thesis will work with the Atari 2600 environments offered by OpenAI \citep{openaigym}

By combining deep learning techniques \citep{Hinton504} with reinforcement learning, the problems posed by most of the Atari games can be solved nowadays.

However, complex environments like the Atari 2600 games can often be costly to simulate.

ACER \citep{ACER} provides a sample efficient learning way to train the agent. This work aims at implementing and evaluating the algorithm.

To lay out the foundation for ACER, we will first discuss core components of reinforcement learning and take a closer look at policy gradient methods, specifically actor-critic methods and the \textit{Advantage Actor Critic} (A3C) - algorithm \citep{A3C}, followed by an overview of different approaches to off-policy learning from experience.

Finally, the ACER algorithm is presented, implemented and evaluated.


\pagebreak