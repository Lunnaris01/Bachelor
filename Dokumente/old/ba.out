\BOOKMARK [1][-]{section.1}{Abstract}{}% 1
\BOOKMARK [1][-]{section.2}{Introduction}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Motivation/Objectives}{section.2}% 3
\BOOKMARK [1][-]{section.3}{Reinforcement Learning Frameworks}{}% 4
\BOOKMARK [2][-]{subsection.3.1}{Elements of Reinforcement Learning}{section.3}% 5
\BOOKMARK [2][-]{subsection.3.2}{Markov Decision Process}{section.3}% 6
\BOOKMARK [1][-]{section.4}{Actor-Critic Methods}{}% 7
\BOOKMARK [2][-]{subsection.4.1}{Actor-Only Methods}{section.4}% 8
\BOOKMARK [2][-]{subsection.4.2}{Critic-Only Methods}{section.4}% 9
\BOOKMARK [2][-]{subsection.4.3}{Actor-Critic Methods}{section.4}% 10
\BOOKMARK [2][-]{subsection.4.4}{A3C : Asynchronous Advantage Actor-Critic}{section.4}% 11
\BOOKMARK [1][-]{section.5}{Off-Policy Learning}{}% 12
\BOOKMARK [2][-]{subsection.5.1}{Importance-Sampling}{section.5}% 13
\BOOKMARK [2][-]{subsection.5.2}{Q-Retrace}{section.5}% 14
\BOOKMARK [1][-]{section.6}{ACER : Actor-Critic with Experience Replay}{}% 15
\BOOKMARK [1][-]{section.7}{Experimental Setup}{}% 16
\BOOKMARK [1][-]{section.8}{Experiments}{}% 17
\BOOKMARK [2][-]{subsection.8.1}{Hyperparameter Robustness}{section.8}% 18
\BOOKMARK [2][-]{subsection.8.2}{Adversial Attacks}{section.8}% 19
\BOOKMARK [2][-]{subsection.8.3}{Results}{section.8}% 20
\BOOKMARK [1][-]{section.9}{Conclusion}{}% 21
\BOOKMARK [1][-]{section.10}{References}{}% 22
\BOOKMARK [1][-]{section*.4}{List of Figures}{}% 23
\BOOKMARK [1][-]{section*.5}{List of Tables}{}% 24
