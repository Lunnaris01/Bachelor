
\begin{algorithm}[h]
{
// \textit{ Assume shared Parameters and counter} $ \theta^{global} ,\ T = 0 $

// \textit{ with local conterparts $\theta$ }

 Initialize parameters for policy $\theta$ and critic $\theta_v$ and counter t=0
 
 \Repeat{T $ >$ T_{max}}
 {
  Reset gradients $d\theta, d\theta_v$
  
 \If{online}
 {
 Synchronize $ \theta, \theta_v$ with $\theta^{global} ,\theta^{global}_v$
 
 get trajectory of length t by interacting with the environment
 
  save trajectory to replay buffer. 
 }
 
 \Else
 {Draw trajectory of length k from replay buffer}
 
 Q^{ret} = \begin{cases} 
 0 &  s_k \text{ terminal} \\
 V_{\theta}(s_k) & s_k \text{ not terminal}
 \end{cases} %]]>
 
 \For{ $i \in \{k -1,\dots, 0\}$ }{
 $Q^{ret} \gets r_i + \gamma Q^{ret}$
 
 $\tilde{p_i} \gets min \{1, \frac{\pi (s_i)}{\mu (s_i)} \}$
\begin{align*}

$g$ \gets \min\{c, p_i(a_i) \} \nabla_\theta log(a_i \mid s_i) (Q^{ret} -V_i) \\
+ \sum_a \left[ 1 - \frac{c}{p_i(a)} \right]_+ \pi(a \mid s_i) \nabla_\theta log \pi(a \mid s_i) (Q_\theta (s_i,a_i) -V_i)
\end{align*}
 Accumulate gradients wrt. $\theta$ :
  $d\theta^{global} \leftarrow d\theta^{global} + g + \nabla_\theta(Q^{ret} - Q_\theta (s_i,a_i))^2$
 
 $Q^{ret} \gets \tilde{p_i} (Q^{ret} - Q_\theta (s_i,a_i)) +V_\theta (s_i) $
 }
 Perform asynchronous update of $\theta^{global}$ and $\theta^{global}_v$ using d$\theta$, d$\theta_v$
 T = T+1
 t=t+1
}
 \If {t \% replay\_ratio == 0}
 {
 online = True
 } 
 \Else
{online = False}
}
\label{ACERALGO}
 \caption{ACER for discrete actions (without TRPO) \citep{ACER}}
\end{algorithm}