\pagebreak
\section{Off-Policy Learning}
\raggedbottom 

Off-policy methods use data sampled from a so called  \textit{behavior policy} we will denote as $\mu(a \mid s)$  to optimize the agents \textit{current/target policy}  $\pi$.

One benefit of off-policy learning is the possibility to chose a more exploratory behaviour policy. 
Another benefit is the possibility to increase sample efficiency by reusing old data.
\citep{Degris12}

Off-policy methods can have the drawback of being divergent. The class of Asynchronous Methods A3C belongs to is always slightly off-policy is only slightly off-policy, which doesn't impact the convergence.

If the behaviour policy can be very different from the target policy, the algorithm can no longer be viewed as \textit{safe}. \citep{Munos16}

This problem is adressed by many different Methods, in order to ensure convergence, even for arbitraty "off-policyness" of sampled data.

\subsection{Importance Sampling (IS)}

One of the most basic ideas is to correct for the "off-policyness" by using Importance sampling.
It is a classic technique for estimating the value of a random variable $x$ with distribution $d$ if the samples were drawn from another distribution $d'$
By using the product of the likelihood ratios 
\begin{align}
{
p_t = \frac{\pi (a_t \mid s_t)}{\mu (a_t \mid s_t)}
}
\label{IW}
\end{align}
Even though this method can guarantee convergence \citep{Munos16}, it comes with the risk of high, possible infinite  variance, due to the variance of the product of importance weights.

\subsection{Tree-backup, TB(\lambda)}

The tree-backup method allows off-policy corrections, without the use of imporance sampling by using the expectation and values under the target policy $\pi$ of every untaken action. \citet{Precup00}

The algorithm provides low variance off-policy learning with strong convergence.
However if a sample is drawn from a policy which is close to the target policy, the algorithm unnessecarly cuts the traces. Without using the full returns, the learning process is slowed down.
\pagebreak
\subsection{Retrace($\lambda$)}

\citet{Munos16} introduced the Retrace($\lambda$) algorithm. By combining ideas of importance sampling and tree-backup, low varience with strong convergence was achieved while keeping the benefits of full returns.

Like TB($\lambda$) the traces are safely cut in case off strong "off-policyness", without impacting the update too much, if the data was sampled under a behaviour policy $\mu$ close to the target policy $\pi$.

Retrace values for a q funcion are obtained recursively by

\begin{align}
{
Q^{ret}(x_t,a_t)=r_t+\gamma \tilde{p}^_{t+1}[Q^{ret}(x_{t+1},a_{t+a} ) -  Q(x_{t+a},a_{t+1})] + \gamma V(x_{t+1})
}
\label{qretrace}
\end{align}

with $\tilde{p} = min\{c,p_t\}$ being the truncated importance weight $p_t$ (\ref{IW}).

In case of a terminal state, the retrace value is equal to the final reward.
Note that the formula is given considering $\lambda = 1$

As $\lambda = 1$ performs the best for the Atari console games \citep{Munos16}, other values were not considered within this Thesis.

\pagebreak
