@book{Sut98,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {2018},
 edition = {2nd},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
 url = {http://incompleteideas.net/book/bookdraft2018mar21.pdf}
} 


@article{Shogi17,
  author    = {David Silver and
               Thomas Hubert and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Matthew Lai and
               Arthur Guez and
               Marc Lanctot and
               Laurent Sifre and
               Dharshan Kumaran and
               Thore Graepel and
               Timothy P. Lillicrap and
               Karen Simonyan and
               Demis Hassabis},
  title     = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
               Learning Algorithm},
  journal   = {CoRR},
  volume    = {abs/1712.01815},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01815},
  archivePrefix = {arXiv},
  eprint    = {1712.01815},
  timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01815},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Article{Go2017,
author={Silver, David 
and Schrittwieser, Julian
and Simonyan, Karen
and Antonoglou, Ioannis
and Huang, Aja
and Guez, Arthur
and Hubert, Thomas
and Baker, Lucas
and Lai, Matthew
and Bolton, Adrian
and Chen, Yutian
and Lillicrap, Timothy
and Hui, Fan
and Sifre, Laurent
and van den Driessche, George
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go without human knowledge},
journal={Nature},
year={2017},
month={Oct},
publisher={Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN  -},
volume={550},
note={Article},
url={http://dx.doi.org/10.1038/nature24270}
}

@misc{OpenAI_dota,
 author = {OpenAI},
 title = {OpenAI Five},
 url = {https://blog.openai.com/openai-five},
 year = {2018}
}

@article{mnih2015atari,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  archivePrefix = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{A3C,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016}
}



@article{ACER,
  author    = {Ziyu Wang and
               Victor Bapst and
               Nicolas Heess and
               Volodymyr Mnih and
               R{\'{e}}mi Munos and
               Koray Kavukcuoglu and
               Nando de Freitas},
  title     = {Sample Efficient Actor-Critic with Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1611.01224},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01224},
  archivePrefix = {arXiv},
  eprint    = {1611.01224},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WangBHMMKF16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article {Hinton504,
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	number = {5786},
	pages = {504--507},
	year = {2006},
	doi = {10.1126/science.1127647},
	publisher = {American Association for the Advancement of Science},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/313/5786/504},
	eprint = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
	journal = {Science}
}


@article{openaigym,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016}
}

@article{nature,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}



@article{deeprlLi,
  author    = {Yuxi Li},
  title     = {Deep Reinforcement Learning: An Overview},
  journal   = {CoRR},
  volume    = {abs/1701.07274},
  year      = {2017}
}

@article{Grondman12,
 author = {Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel A. D. and Babuska, Robert},
 title = {A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients},
 journal = {Trans. Sys. Man Cyber Part C},
 issue_date = {November 2012},
 volume = {42},
 number = {6},
 month = nov,
 year = {2012},
 issn = {1094-6977},
 pages = {1291--1307},
 numpages = {17},
 url = {https://doi.org/10.1109/TSMCC.2012.2218595},
 doi = {10.1109/TSMCC.2012.2218595},
 acmid = {2719775},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA}
} 

@Article{Williams1992,
author="Williams, Ronald J.",
title="Simple statistical gradient-following algorithms for connectionist reinforcement learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="229--256",
abstract="This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.",
issn="1573-0565",
doi="10.1007/BF00992696",
url="https://doi.org/10.1007/BF00992696"
}

@INPROCEEDINGS{Sutton00policygradient,
    author = {Richard S. Sutton and David Mcallester and Satinder Singh and Yishay Mansour},
    title = {Policy gradient methods for reinforcement learning with function approximation},
    booktitle = {In Advances in Neural Information Processing Systems 12},
    year = {2000},
    pages = {1057--1063},
    publisher = {MIT Press}
}


@article{Munos16,
  author    = {R{\'{e}}mi Munos and
               Tom Stepleton and
               Anna Harutyunyan and
               Marc G. Bellemare},
  title     = {Safe and Efficient Off-Policy Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1606.02647},
  year      = {2016}
}



@article{Degris12,
  author    = {Thomas Degris and
               Martha White and
               Richard S. Sutton},
  title     = {Off-Policy Actor-Critic},
  journal   = {CoRR},
  volume    = {abs/1205.4839},
  year      = {2012}
}

@article{Precup00,
 author = {Precup, Doina and Sutton, Richard S. and Singh, Satinder P.},
 title = {Eligibility Traces for Off-Policy Policy Evaluation},
 booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
 series = {ICML '00},
 year = {2000},
 isbn = {1-55860-707-2},
 pages = {759--766},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=645529.658134},
 acmid = {658134},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA}
} 



@article{cnn,
  author    = {Keiron O'Shea and
               Ryan Nash},
  title     = {An Introduction to Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1511.08458},
  year      = {2015}
}

 @misc{karpathy, title={Deep Reinforcement Learning: Pong from Pixels}, url={http://karpathy.github.io/2016/05/31/rl/}, journal={Deep Reinforcement Learning: Pong from Pixels}, author={Karpathy, Andrej}} 
 
 

@article{Schulman15,
  author    = {John Schulman and
               Philipp Moritz and
               Sergey Levine and
               Michael I. Jordan and
               Pieter Abbeel},
  title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  journal   = {CoRR},
  volume    = {abs/1506.02438},
  year      = {2015}
}
@article{entropy,
author = {Jost, Lou},
title = {Entropy and diversity},
journal = {Oikos},
volume = {113},
number = {2},
pages = {363-375},
doi = {10.1111/j.2006.0030-1299.14714.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2006.0030-1299.14714.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2006.0030-1299.14714.x},
abstract = {Entropies such as the Shannon–Wiener and Gini–Simpson indices are not themselves diversities. Conversion of these to effective number of species is the key to a unified and intuitive interpretation of diversity. Effective numbers of species derived from standard diversity indices share a common set of intuitive mathematical properties and behave as one would expect of a diversity, while raw indices do not. Contrary to Keylock, the lack of concavity of effective numbers of species is irrelevant as long as they are used as transformations of concave alpha, beta, and gamma entropies. The practical importance of this transformation is demonstrated by applying it to a popular community similarity measure based on raw diversity indices or entropies. The standard similarity measure based on untransformed indices is shown to give misleading results, but transforming the indices or entropies to effective numbers of species produces a stable, easily interpreted, sensitive general similarity measure. General overlap measures derived from this transformed similarity measure yield the Jaccard index, Sørensen index, Horn index of overlap, and the Morisita–Horn index as special cases.}
}

