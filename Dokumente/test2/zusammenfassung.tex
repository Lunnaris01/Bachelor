%%% Die folgende Zeile nicht Ã¤ndern!
\section*{\ifthenelse{\equal{\sprache}{deutsch}}{Zusammenfassung}{Abstract}}
%%% Zusammenfassung:
Deep reinforcement learning and policy gradient methods majorly contributed to the most recent advances in the field of Artificial Intelligence.
These methods enable machines to surpass human performance for Atari console games \citep{mnih2015atari}, board games like Chess, Shogi \citep{Shogi17} or Go \citep{Go2017} and most recently even complex team-based computer games \citep{OpenAI_dota}.

As environments get more complex the cost of simulating the environment increases and often outweighs the isolated computational cost of training the agent, making sample efficient methods necessary.

This thesis will take a look at off-policy methods and learning from previously sampled data, with the main focus being the implementation and evaluation of the "Actor-Critic with Experience Replay" (ACER) algorithm proposed by \citet{ACER} on the Atari 2600 console games.
